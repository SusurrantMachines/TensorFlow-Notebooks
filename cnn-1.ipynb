{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Simple CNN for notMNIST\n",
    "============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a basic four-layer convolutional neural network originally taken from Udacity's deep learning course's *Assignment 4*.  I heavily reorganized it, added some extra machine learning tricks, updated it for TensorFlow 1.0, and replaced some pieces of it with bits of Google's *cifar10.py* code.\n",
    "\n",
    "In version 4.4 I'll add more layers and turn it into LeNet-5.\n",
    "\n",
    "In version 4.5 I'll add some more fancy tricks like (as the assignment suggests) dropout and learning rate decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data from a pickle I made earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11948,
     "status": "ok",
     "timestamp": 1446658914837,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "016b1a51-0290-4b08-efdb-8c95ffc3cd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (18724, 28, 28) (18724,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valdn_dataset = save['valid_dataset']\n",
    "    valdn_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valdn_dataset.shape, valdn_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "### Preprocessing: Normalization\n",
    "Apparently, these networks learn better if the input data (the pixel values, in this context) have a mean of 0 and a variance of 1.  So, we want to adjust the pixel values a bit before we dump them into the machine (currently they're integers ranging from 0 to 255 representing greyscale values).\n",
    "\n",
    "Let $K = N \\times H \\times W$, where $N$ is the number of images, $H$ is the height of each image, and $W$ is the width.  Let $x_1, x_2, \\ldots, x_K$ be the big list of all the pixel values from all the training images.\n",
    "\n",
    "Getting a mean of 0 is easy.  Just compute the mean $\\mu$ of the pixels in the training data and subtract that from each of them.  We'll also need treat that $\\mu$ as an estimator of the mean of our other datasets and subtract it from them too.\n",
    "\n",
    "But how do we get varianace 1?  Well, after that first step, our new pixel values will be $x_i' = (x_i-\\mu)$.  We can adjust the variance of those pixels by scaling them.  So, we're looking for a scalar $c$ that will make those values $c(x_i-\\mu)$ have a variance of 1.  The variance of these adjusted pixels is given by this expression (where $\\sigma^2$ is the variance of the original pixels):\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "\\sum_1^N \\left(c \\left(x_i - \\mu\\right)\\right)^2\n",
    "&=& c^2 \\sum_1^N \\left(x_i - \\mu\\right)^2\\\\\n",
    "&=& c^2 \\sigma^2\\\\\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "So, there you go.  Our scalar $c = \\frac 1 \\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find mean of training data\n",
    "mu = np.mean(train_dataset)\n",
    "\n",
    "# Use mu as an estimator of the mean of all datasets and subtract it\n",
    "train_dataset = train_dataset - mu\n",
    "valdn_dataset = valdn_dataset - mu\n",
    "test_dataset = test_dataset - mu\n",
    "\n",
    "# Find standard deviation of training data\n",
    "sigma = np.std(train_dataset)\n",
    "\n",
    "# Divide by sigma to get training variance of 1 (and adjust other datasets)\n",
    "train_dataset = train_dataset / sigma\n",
    "valdn_dataset = valdn_dataset / sigma\n",
    "test_dataset = test_dataset / sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000000 <-- should be 0\n",
      "0.005162 <-- should be close to 0\n",
      "0.015914 <-- should be close to 0\n",
      "1.000001 <-- should be 1\n",
      "1.002455 <-- should be close to 1\n",
      "1.019278 <-- should be close to 1\n"
     ]
    }
   ],
   "source": [
    "print(\"%f <-- should be 0\" % np.mean(train_dataset))\n",
    "print(\"%f <-- should be close to 0\" % np.mean(valdn_dataset)) \n",
    "print(\"%f <-- should be close to 0\" % np.mean(test_dataset)) \n",
    "print(\"%f <-- should be 1\" % np.var(train_dataset))\n",
    "print(\"%f <-- should be close to 1\" % np.var(valdn_dataset)) \n",
    "print(\"%f <-- should be close to 1\" % np.var(test_dataset)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "### Preprocessing: Reformat data into a TensorFlow-friendly shapes\n",
    "- The design matrix is currently N x H x W, since they're greyscale images (no channel dimension like you'd get with colour images).  tf.nn.conv2d needs them formatted as N x H x W x #channels, however.  So we need to add a dimension at the end of the shape vector for the input data.\n",
    "- Labels need to be one-hot encoded floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, num_categories):\n",
    "    '''Converts an array of labels to an array of one-hot-encoded labels.\n",
    "    [NB: Just noticed there's now a \"tf.one_hot\" function.  Should switch to that.]\n",
    "    Args:\n",
    "        labels: list (row vector) of N labels (where N is the number of examples)\n",
    "        num_categories: the number of possible labels\n",
    "    Returns:\n",
    "        N x num_categories array of one-hot encoded labels\n",
    "    '''\n",
    "    # IMPLEMENTATION\n",
    "    # np.expand_dims(labels, 1) is an Nx1 array (a.k.a. a column vector) \n",
    "    # [0..num_categories] == k is evaluated for each label k in the column vector\n",
    "    # This produces a one-hot vector [false, false, ..., true, false, ..., false] where \"true\" is in position k\n",
    "    # Then astype(float32) converts the array of booleans to an array of 0s and 1s (as floats)\n",
    "    return (np.arange(num_categories) == np.expand_dims(labels, 1)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretend labels =  [5 6 6 4 8]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing one_hot_encode\n",
    "pretend_labels = np.random.randint(10, size=5)\n",
    "print(\"pretend labels = \", pretend_labels)\n",
    "one_hot_encode(pretend_labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11952,
     "status": "ok",
     "timestamp": 1446658914857,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset\t (200000, 28, 28, 1) \tTraining labels\t\t (200000, 10)\n",
      "Validation dataset\t (10000, 28, 28, 1) \tValidation labels\t (10000, 10)\n",
      "Test dataset\t\t (18724, 28, 28, 1) \tTesting labels\t\t (18724, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28  # 32 for MNIST, but we're doing notMNIST\n",
    "num_labels = 10  # notMNIST is ABCDEFGHIJ, which is also 10 categories\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "with tf.name_scope(\"Training-Input\") as scope:\n",
    "    train_dataset = np.expand_dims(train_dataset, 3) # NxHxW --> NxHxWx1\n",
    "    train_labels = one_hot_encode(train_labels, num_labels)\n",
    "with tf.name_scope(\"Validation-Input\") as scope:\n",
    "    valdn_dataset = np.expand_dims(valdn_dataset, 3) # NxHxW --> NxHxWx1\n",
    "    valdn_labels = one_hot_encode(valdn_labels, num_labels)\n",
    "with tf.name_scope(\"Testing-Input\") as scope:\n",
    "    test_dataset = np.expand_dims(test_dataset, 3) # NxHxW --> NxHxWx1\n",
    "    test_labels = one_hot_encode(test_labels, num_labels)\n",
    "print('Training dataset\\t', train_dataset.shape, \"\\tTraining labels\\t\\t\", train_labels.shape)\n",
    "print('Validation dataset\\t', valdn_dataset.shape, \"\\tValidation labels\\t\", valdn_labels.shape)\n",
    "print('Test dataset\\t\\t', test_dataset.shape, \"\\tTesting labels\\t\\t\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Computation Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Hyperparameters\") as scope:\n",
    "    batch_size = 16\n",
    "    patch_size = 5    # kernel \n",
    "    depth = 16        # depth of first hidden layer\n",
    "    num_hidden = 64\n",
    "    test_batch_size = 1000  # no need to overthink think this one; just find a size that fits on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Graph and Set Up Placeholder Nodes in GPU Memory for the Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.name_scope('Training-Input-Data'):\n",
    "        tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "        tf_train_labels  = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    with tf.name_scope('Validation-Input-Data'):\n",
    "        tf_valdn_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "        tf_valdn_labels  = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    with tf.name_scope('Test-Input-Data'):\n",
    "        tf_test_dataset = tf.placeholder(tf.float32, shape=(test_batch_size, image_size, image_size, num_channels))\n",
    "        tf_test_labels  = tf.placeholder(tf.float32, shape=(test_batch_size, num_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allocate and Initialize All Weights and Biases\n",
    "We'll use tf.get_variable instead of tf.Variable to make it easier for multi-GPU runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    \n",
    "    # First Conv Layer\n",
    "    with tf.variable_scope('ConvPoolRelu1'):\n",
    "        weights = tf.get_variable(name = \"weights\", \n",
    "                                  shape = (patch_size, patch_size, num_channels, depth), \n",
    "                                  initializer = tf.truncated_normal_initializer(mean = 0, \n",
    "                                                                               stddev = 0.1, \n",
    "                                                                               seed = None), \n",
    "                                  dtype = tf.float32)\n",
    "        biases = tf.get_variable(name = \"biases\",\n",
    "                                 shape = (depth),\n",
    "                                 initializer = tf.constant_initializer(0),\n",
    "                                 dtype = tf.float32)\n",
    "    # Second Conv Layer\n",
    "    with tf.variable_scope('ConvPoolRelu2'):  \n",
    "        weights = tf.get_variable(name = \"weights\", \n",
    "                                  shape = (patch_size, patch_size, depth, depth), \n",
    "                                  initializer = tf.truncated_normal_initializer(mean = 0, \n",
    "                                                                               stddev = 0.1, \n",
    "                                                                               seed = None), \n",
    "                                  dtype = tf.float32)\n",
    "        biases = tf.get_variable(name = \"biases\",\n",
    "                                 shape = (depth),\n",
    "                                 initializer = tf.constant_initializer(0),\n",
    "                                 dtype = tf.float32)\n",
    "    # First Fully-Connected Layer\n",
    "    with tf.variable_scope('FullyConnected3'):\n",
    "        weights = tf.get_variable(name = \"weights\", \n",
    "                                  shape = ((image_size//4)**2 * depth, num_hidden), \n",
    "                                  initializer = tf.truncated_normal_initializer(mean = 0, \n",
    "                                                                               stddev = 0.1, \n",
    "                                                                               seed = None), \n",
    "                                  dtype = tf.float32)\n",
    "        biases = tf.get_variable(name = \"biases\",\n",
    "                                 shape = (num_hidden),\n",
    "                                 initializer = tf.constant_initializer(0),\n",
    "                                 dtype = tf.float32)\n",
    "    # Second Fully-Connected Layer\n",
    "    with tf.variable_scope('FullyConnected4'):\n",
    "        weights = tf.get_variable(name = \"weights\", \n",
    "                                  shape = (num_hidden, num_labels), \n",
    "                                  initializer = tf.truncated_normal_initializer(mean = 0, \n",
    "                                                                               stddev = 0.1, \n",
    "                                                                               seed = None), \n",
    "                                  dtype = tf.float32)\n",
    "        biases = tf.get_variable(name = \"biases\",\n",
    "                                 shape = (num_labels),\n",
    "                                 initializer = tf.constant_initializer(0),\n",
    "                                 dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer-building Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_pool_relu_layer(X, W, b):\n",
    "    conv_X = tf.nn.conv2d(X, W, [1, 1, 1, 1], padding='SAME')\n",
    "    pool_conv_X = tf.nn.max_pool(conv_X, [1,2,2,1], [1,2,2,1], padding='SAME')\n",
    "    relu_pool_conv_X = tf.nn.relu(pool_conv_X + b)\n",
    "    return relu_pool_conv_X\n",
    "\n",
    "def fc_layer(X, W, b):\n",
    "    return tf.matmul(X, W) + b\n",
    "\n",
    "def flatten_activation_map(x):\n",
    "    \"\"\"Converts a 4D tensor into a 2D tensor (so it can be fed to a fully-connected layer).\n",
    "    Args:\n",
    "        x: tensor with shape (N, H, W, C)\n",
    "    Returns:\n",
    "        tensor with shape (N, H*W*C)\n",
    "    \"\"\"\n",
    "    s = x.get_shape().as_list()\n",
    "    return tf.reshape(x, [s[0], -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network-assembly Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(data):\n",
    "    \"\"\"Retrieves the weights and biases and constructs the computation graph with them.\n",
    "    Args:\n",
    "        data: A batch tensor of input examples K x H x W x D (where K is the batch size)\n",
    "    Returns: the network output K x C (where C is the number of categories)\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('ConvPoolRelu1', reuse=True):\n",
    "        W = tf.get_variable(\"weights\")\n",
    "        B = tf.get_variable(\"biases\")\n",
    "        layer_1_output = conv_pool_relu_layer(data, W, B)\n",
    "    with tf.variable_scope('ConvPoolRelu2', reuse=True):\n",
    "        W = tf.get_variable(\"weights\")\n",
    "        B = tf.get_variable(\"biases\")\n",
    "        layer_2_output = conv_pool_relu_layer(layer_1_output, W, B)\n",
    "        flat_X = flatten_activation_map(layer_2_output)\n",
    "    with tf.variable_scope('FullyConnected3', reuse=True):\n",
    "        W = tf.get_variable(\"weights\")\n",
    "        B = tf.get_variable(\"biases\")\n",
    "        fc1_layer_output = tf.nn.relu(fc_layer(flat_X, W, B))\n",
    "    with tf.variable_scope('FullyConnected4', reuse=True):\n",
    "        W = tf.get_variable(\"weights\")\n",
    "        B = tf.get_variable(\"biases\")\n",
    "        fc2_layer_output = fc_layer(fc1_layer_output, W, B)\n",
    "    return fc2_layer_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret the Output, Set Up the Loss Function, Specify the Learning Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "IZYv70SvvOan"
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "    tf.summary.scalar(\"Loss\", loss)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valdn_prediction = tf.nn.softmax(model(tf_valdn_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training-related Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "AgQDIREv02p1"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    '''\n",
    "    \"Grades\" a list of predictions (like marking a multiple choice test).\n",
    "    Accepts a list of one-hot predictions and a list of ground truth labels (the right answers) \n",
    "    Args:\n",
    "        predictions: NxK float array of one-hot-encoded network output, \n",
    "                     where N is the number of examples and K is the number of categories\n",
    "        labels: NxK array of one-hot-encoded ground truth labels (why one-hot encode those??)\n",
    "    Returns:\n",
    "        Percetage of correct predictions.\n",
    "    '''\n",
    "    # argmax(predictions, 1) gives the \"mostly likely\" category (\"one-decodes\" the network output)\n",
    "    # argmax(labels, 1) does the same thing \n",
    "    #    Why did we even one-hot encode those to begin with??\n",
    "    #    Ah... Right.  Because the loss function compares them to the network output.\n",
    "    # By putting the == inside a sum, we're implicitly casting the equality test to an int.\n",
    "    # Correct predictions yield 1, incorrect ones yield 0.\n",
    "    # Then we divide the tally of 1s by the number of predictions in the set.\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) \n",
    "          / predictions.shape[0])\n",
    "\n",
    "def variable_summaries(var, name):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor.\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean/' + name, mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))\n",
    "        tf.summary.scalar('sttdev/' + name, stddev)\n",
    "        tf.summary.scalar('max/' + name, tf.reduce_max(var))\n",
    "        tf.summary.scalar('min/' + name, tf.reduce_min(var))\n",
    "        tf.summary.histogram(name, var)\n",
    "    \n",
    "def clear_tensorboard_dir(path):\n",
    "    if tf.gfile.Exists(path):\n",
    "        tf.gfile.DeleteRecursively(path)\n",
    "    tf.gfile.MakeDirs(path)\n",
    "\n",
    "def make_batch(data, max_size, batch_num):\n",
    "    n = data.shape[0]\n",
    "    offset = (batch_num * max_size) % n\n",
    "    return data[offset: offset+max_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conduct the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "..................................................n_test_cases = 18724\n",
      "n_batches =  18\n",
      "Test accuracy: 90.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5000\n",
    "\n",
    "# Clear the tensorboard directory\n",
    "clear_tensorboard_dir(\"tensorboard\")\n",
    "\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.allocator_type = 'BFC'\n",
    "# with tf.Session(config = config, graph=graph) as session:\n",
    "with tf.Session(graph=graph) as session:\n",
    "    merged_summaries = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter('./tensorboard', session.graph)\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Training...')\n",
    "    for step in range(num_steps):\n",
    "        batch_data = make_batch(train_dataset, batch_size, step)\n",
    "        batch_labels = make_batch(train_labels, batch_size, step)\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        merged_sums, _, l, predictions = session.run([merged_summaries, optimizer, loss, train_prediction], \n",
    "                                                     feed_dict=feed_dict)\n",
    "        if (step % 100 == 0):\n",
    "            train_writer.add_summary(merged_sums, step)\n",
    "            print('.', end=\"\")\n",
    "    \n",
    "    #--------------------- Testing Phase -------------------\n",
    "    total_accuracy = 0\n",
    "    n_test_cases = int(test_dataset.shape[0])\n",
    "    print(\"n_test_cases = %d\" % n_test_cases)\n",
    "    n_batches = int(n_test_cases / test_batch_size)\n",
    "    print(\"n_batches = \", str(n_batches))\n",
    "    for step in range(n_batches):\n",
    "        batch_data = make_batch(test_dataset, test_batch_size, step)\n",
    "        batch_labels = make_batch(test_labels, test_batch_size, step)\n",
    "        feed_dict = {tf_test_dataset: batch_data, tf_test_labels: batch_labels}\n",
    "        test_output_batch = session.run([test_prediction], feed_dict=feed_dict)[0]\n",
    "        this_batch_size = float(batch_labels.shape[0])\n",
    "        batch_accuracy = accuracy(np.asarray(test_output_batch), np.asarray(batch_labels))\n",
    "        total_accuracy = total_accuracy + batch_accuracy * this_batch_size / float(n_test_cases)\n",
    "\n",
    "    print('Test accuracy: %.1f%%' % total_accuracy)\n",
    "    train_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
