{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV Input Pipeline for an $\\mathbb R^2$ Binary Classifier  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- Construst a robust input pipeline to read in the CSV files generated by *generate_data-1.ipynb*.\n",
    "- Use TensorFlow's `QueueRunners` to prepare shuffled mini-batches of examples from those files.\n",
    "- Flesh out the skeletal framework from *PausableTraining.ipynb* with a pauseable logistic regression classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Input Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Don't run this notebook until you've run *generate_data-1.ipynb* at least once first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "notebook_dir = \"cluster2D\"\n",
    "g = tf.Graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch Design Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the list of .csv files in the `notebook_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PausableLogisticRegression/1.csv',\n",
       " 'PausableLogisticRegression/3.csv',\n",
       " 'PausableLogisticRegression/2.csv',\n",
       " 'PausableLogisticRegression/0.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file_list = [os.path.join(notebook_dir, x) for x in os.listdir(notebook_dir) if x.endswith('.csv')]\n",
    "csv_file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add operations to the graph which prepare mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with g.as_default():\n",
    "    # Create a queue of the filenames\n",
    "    filename_queue = tf.train.string_input_producer(csv_file_list)\n",
    "    \n",
    "    # (if there are lines of metadata at the top of the file, we can skip them using this parameter)\n",
    "    reader = tf.TextLineReader(skip_header_lines=0)\n",
    "    \n",
    "    # Create graph operations to read a line from a CSV file (key is optional, but yields good info)\n",
    "    key, line_from_file = reader.read(filename_queue)\n",
    "    \n",
    "    # Specify the default field values (in case they're missing) and their type\n",
    "    record_defaults = [[0.5], [0.5], [1.0], [tf.to_int32(0)]]\n",
    "    \n",
    "    # Parse the CSV string into an example tensor (x,y,w,label)\n",
    "    example = tf.decode_csv(line_from_file, record_defaults=record_defaults)\n",
    "    \n",
    "    # Split the example into a feature vector and a label\n",
    "    features = tf.stack(example[:3])\n",
    "    label = example[-1]\n",
    "    \n",
    "    # Graph operations that produce a mini-batch (key batch is optional)\n",
    "    key_batch, data_batch, label_batch = tf.train.shuffle_batch(\n",
    "        [key, features, label], \n",
    "        batch_size=10, \n",
    "        capacity=400, \n",
    "        min_after_dequeue=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot going on in this block and the documentation for these functions isn't totally clear, but if you play with the parameters of `tf.train.shuffle_batch` and watch that `key` variable, it quickly becomes apparent what's going on:\n",
    "- Producer threads continually read examples from the files and dump them into a bin (a random queue) unless the bin contains `capacity` examples (in which case they wait until it doesn't).\n",
    "- Consumer threads remove randomly selected `batch_sized` examples from the bin unless it contains < `batch_size + min_after_dequeue` examples.\n",
    "- You can make mini-batches indefinitely (the data are re-read).\n",
    "- Each time `tf.train.shuffle_batch` is called a file is randomly chosen and examples are read from it **in sequence**, starting from the first.\n",
    "- If the same file is opened multiple times, the reader doesn't pick up where it left off: it reads from the beginning again!  (So, make sure `batch_size + min_after_dequeue` $\\geq$ #examples/file)\n",
    "\n",
    "Particularly useful tests:\n",
    "- If you set `min_after_dequeue=0` then each mini-batch contains the first `batch_size` examples in order (because we gave `shuffle_batch` permission to empty the queue without waiting for more than one example).  This means we have to set `min_after_dequeue` $\\geq$ number of examples in each file (otherwise the examples at the end of each file will never appear in a mini-batch).\n",
    "- If you set `min_after_dequeue=1` then two examples are tossed in the bin and one of them is drawn at random and replaced by the next example in the file.  Then we choose another one at random.\n",
    "\n",
    "**Question:** Could I use `reader.read_up_to()` instead of `reader.read()` here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's have a quick look at some of these mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'PausableLogisticRegression/1.csv:6'\n",
      " b'PausableLogisticRegression/1.csv:1'\n",
      " b'PausableLogisticRegression/1.csv:5'\n",
      " b'PausableLogisticRegression/2.csv:2'\n",
      " b'PausableLogisticRegression/1.csv:24'\n",
      " b'PausableLogisticRegression/3.csv:2'\n",
      " b'PausableLogisticRegression/2.csv:22'\n",
      " b'PausableLogisticRegression/3.csv:8'\n",
      " b'PausableLogisticRegression/3.csv:11'\n",
      " b'PausableLogisticRegression/2.csv:13'] [[-0.31064299  0.490621    1.        ]\n",
      " [-0.57483     0.42409301  1.        ]\n",
      " [-0.116197   -0.40730399  1.        ]\n",
      " [ 0.29039899  1.14159     1.        ]\n",
      " [-0.597516    0.380602    1.        ]\n",
      " [ 1.15666997  1.26133001  1.        ]\n",
      " [ 0.91725898  0.87689102  1.        ]\n",
      " [ 0.63346601  0.95928502  1.        ]\n",
      " [ 0.95553797  0.98903     1.        ]\n",
      " [ 0.763758    0.49739     1.        ]] [0 0 0 1 0 1 1 1 1 1]\n",
      "[b'PausableLogisticRegression/3.csv:23'\n",
      " b'PausableLogisticRegression/0.csv:10'\n",
      " b'PausableLogisticRegression/2.csv:8'\n",
      " b'PausableLogisticRegression/0.csv:5'\n",
      " b'PausableLogisticRegression/1.csv:14'\n",
      " b'PausableLogisticRegression/0.csv:9'\n",
      " b'PausableLogisticRegression/1.csv:8'\n",
      " b'PausableLogisticRegression/1.csv:2'\n",
      " b'PausableLogisticRegression/1.csv:4'\n",
      " b'PausableLogisticRegression/0.csv:12'] [[ 1.28059006  0.76981401  1.        ]\n",
      " [-0.71055198  0.0510946   1.        ]\n",
      " [ 0.82052499  1.45641005  1.        ]\n",
      " [ 0.246833   -0.137127    1.        ]\n",
      " [-0.0621773   0.193698    1.        ]\n",
      " [ 0.74513298  0.148958    1.        ]\n",
      " [-0.71673     0.37914401  1.        ]\n",
      " [-0.26003301 -0.406701    1.        ]\n",
      " [ 0.103914   -1.09117997  1.        ]\n",
      " [ 0.31713501 -0.0118351   1.        ]] [1 0 1 0 0 0 0 0 0 0]\n",
      "[b'PausableLogisticRegression/3.csv:20'\n",
      " b'PausableLogisticRegression/0.csv:25'\n",
      " b'PausableLogisticRegression/3.csv:1'\n",
      " b'PausableLogisticRegression/2.csv:10'\n",
      " b'PausableLogisticRegression/0.csv:13'\n",
      " b'PausableLogisticRegression/2.csv:8'\n",
      " b'PausableLogisticRegression/0.csv:21'\n",
      " b'PausableLogisticRegression/2.csv:25'\n",
      " b'PausableLogisticRegression/3.csv:6'\n",
      " b'PausableLogisticRegression/2.csv:24'] [[ 1.19340003  1.50150001  1.        ]\n",
      " [ 0.50928199 -0.72713202  1.        ]\n",
      " [-0.235522    0.93222302  1.        ]\n",
      " [ 1.08472002  0.16844299  1.        ]\n",
      " [-0.35455599 -0.191044    1.        ]\n",
      " [ 0.82052499  1.45641005  1.        ]\n",
      " [ 0.496995   -1.07455003  1.        ]\n",
      " [ 1.21108997  1.31614006  1.        ]\n",
      " [ 0.73039901  0.83127499  1.        ]\n",
      " [ 0.56112403  1.51766002  1.        ]] [1 0 1 1 0 1 0 1 1 1]\n",
      "[b'PausableLogisticRegression/1.csv:1'\n",
      " b'PausableLogisticRegression/0.csv:4'\n",
      " b'PausableLogisticRegression/0.csv:5'\n",
      " b'PausableLogisticRegression/2.csv:18'\n",
      " b'PausableLogisticRegression/3.csv:10'\n",
      " b'PausableLogisticRegression/2.csv:2'\n",
      " b'PausableLogisticRegression/3.csv:12'\n",
      " b'PausableLogisticRegression/3.csv:14'\n",
      " b'PausableLogisticRegression/2.csv:11'\n",
      " b'PausableLogisticRegression/1.csv:24'] [[-0.57483     0.42409301  1.        ]\n",
      " [-0.0692418   0.369407    1.        ]\n",
      " [ 0.246833   -0.137127    1.        ]\n",
      " [ 0.81400901  1.33754003  1.        ]\n",
      " [ 0.29993901  1.45545006  1.        ]\n",
      " [ 0.29039899  1.14159     1.        ]\n",
      " [ 1.58662999  1.43774998  1.        ]\n",
      " [ 0.63241398  0.91203499  1.        ]\n",
      " [ 1.75751996  2.08857989  1.        ]\n",
      " [-0.597516    0.380602    1.        ]] [0 0 0 1 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "    \n",
    "    for i in range(2000):\n",
    "        k, b, l = sess.run([key_batch, data_batch, label_batch])\n",
    "        if i % 500 == 0:\n",
    "            print(k, b, l)\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In logistic regression over a plane, our trainable model is a vector $\\boldsymbol \\theta \\in \\mathbb R^3$ of parameters that uniquely specifies a separating hyperplane.  Our input is a set of points in $\\mathbf p \\in \\mathbb R^2$ that we tabulate in an augmented design matrix $X \\in \\mathbb R^{N\\times 3}$ where each row is of the form $(x_1,x_2,1)$.  The last dimension is held at a constant value of 1 in order to support a handy trick: translate points using matrix multiplication (instead of vector addition).\n",
    "\n",
    "Our (parametrized) hypothesis function $h_{\\boldsymbol \\theta} (\\mathbf p)$ represents the certainty with which the classifier believes $\\mathbf p$ to be red instead of blue.  For example, if $h_{\\boldsymbol \\theta} (\\mathbf q)=0.25$, that means the classifier is 75% sure that $\\mathbf q$ is blue).  Hence, we can write\n",
    "$$\n",
    "\\begin{align}\n",
    "P(y = 1 \\mid \\mathbf p; \\boldsymbol \\theta) &= h_{\\boldsymbol \\theta} (\\mathbf p)\\\\\n",
    "P(y = 0 \\mid \\mathbf p; \\boldsymbol \\theta) &= 1-h_{\\boldsymbol \\theta} (\\mathbf p)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "These can be condensed into a single expression,\n",
    "$$\n",
    "\\begin{align}\n",
    "P(y \\mid \\mathbf p; \\boldsymbol \\theta) &= \\left(h_{\\boldsymbol \\theta} (\\mathbf p)\\right)^y \\left(1-h_{\\boldsymbol \\theta} (\\mathbf p)\\right)^{(1-y)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The core of the hypothesis function is just the the linear functional $\\varphi_\\theta : \\mathbb R^3 \\rightarrow \\mathbb R$, given by $$\\varphi_\\theta  = \\left| \\, \\boldsymbol \\theta \\right\\rangle$$\n",
    "\n",
    "If $\\varphi_\\theta(\\mathbf p) = \\mathbf p \\cdot \\boldsymbol \\theta$ is negative, the classifer guesses $\\mathbf p$ is blue; if it's positive the classifier guesses $\\mathbf p$ is red.  If it's 0, the classifier doesn't know what to think! \n",
    "\n",
    "The range of $\\varphi_\\theta$ is all of $\\mathbb R$, however, so it doesn't make sense to treat it as a probability directly.  In order to do that, we need to temper it with a function that will compress all of $\\mathbb R$ down into the unit interval (where probabilities live).  There are all sorts of functions that could do this, but since we're going to use gradient descent we need one which is monotonic, differentiable, and whose derivative never vanishes (because derivatives of 0 thwart gradient descent--this is called \"saturation\").  Furthermore, it would be wonderful if it had a derivative that was super-easy to calculate.\n",
    "\n",
    "The logistic sigmoid fits the bill perfectly. Here it is with its derivative: $$s(t)=\\frac 1 {1+e^{-t}},\\ \\ \\ \\ \\ \\ \\frac d {dt} s(t) = s(t)(1-s(t))$$\n",
    "\n",
    "Isn't that derivative amazing and beautiful?  Thus, our hypothesis function is\n",
    "$$\n",
    "h_{\\boldsymbol \\theta} (\\mathbf p) = s\\left(\\varphi_\\theta(\\mathbf p)\\right) = \\frac 1 {1+e^{-\\langle \\boldsymbol \\theta, \\mathbf p\\rangle}} = \\frac 1 {1+e^{-\\theta_0 -\\theta_1 x_1 - \\theta_2 x_2}}\n",
    "$$\n",
    "\n",
    "Given a ground truth label $y \\in \\{0,1\\}^N$ (0 for \"blue\", 1 for \"red\") for point $\\mathbf p$, the cross-entropy between $y$ and our estimator $h_{\\boldsymbol \\theta} (\\mathbf p)$ is given by:\n",
    "$$\n",
    "\\begin{align*}\n",
    "H(y,h_{\\boldsymbol \\theta} (\\mathbf p)) &= -\\sum_{i=0}^1 P(y=i \\mid y) \\log P(y=i \\mid \\mathbf p; \\boldsymbol \\theta)\\\\\n",
    "&= -(1-y) \\log P(y=0 \\mid \\mathbf p; \\boldsymbol \\theta) - y \\log P(y=1 \\mid \\mathbf p; \\boldsymbol \\theta)\\\\\n",
    "&= -(1-y) \\log (1-h_{\\boldsymbol \\theta} (\\mathbf p)) - y \\log h_{\\boldsymbol \\theta} (\\mathbf p)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This is for batch gradient descent, however.  For stochastic gradient descent, we're looking at only a single training example $(\\mathbf p, y) \\in \\mathbb R^3 \\times \\mathbb 2$.  The cross-entropy loss in this case is \n",
    "$$\n",
    "H(y,h_{\\boldsymbol \\theta} (\\mathbf p)) = -y \\log h_{\\boldsymbol \\theta} (\\mathbf p)\n",
    "$$\n",
    "\n",
    "For any type of gradient descent we need the gradient of the loss:\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\nabla_\\theta H(y,h_{\\boldsymbol \\theta} (\\mathbf p)) = -y \\log h_{\\boldsymbol \\theta} (\\mathbf p)\n",
    "\\end{align*}\n",
    "$$\n",
    "$$\\vdots$$\n",
    "\n",
    "Actually, you know what?  This whole derivation has blown, like, seriously over-quota.  Let's cut to the chase already and relegate all this thoroughly edge-of-your-seat calculus to the appendix.  Let me just say this (this is the really important implementation issue):\n",
    "\n",
    "During training, we need to compute the gradient of the loss.  Now, the loss is the cross-entropy between the sigmoid/softmax of an inner product (i.e. the classifier's prediction) and the \"real,\" ground truth labels.  So, to calculate that, first we have to compute the the classifier's prediction, then the cross-entropy, and then take the gradient, right?\n",
    "\n",
    "BZZZZZ!  Wrong!\n",
    "\n",
    "That's doing a whole lot of inefficient busy-work that, as an added bonus, is quite likely to give you underflows, overflows, and truncation errors (e.g. think about dividing a float by, say, $10^{-16}$ and then multiplying it by $10^{-16}$).  The secret is that the gradient of a sigmoid is really simple and a cross entropy of a sigmoid involves some logarithms of exponential functions that can be gracefully cancelled-out as long as you skip the intermediate steps and calculate the gradient directly.  \n",
    "\n",
    "All we care about during training is the gradient anyway, so we can get away with doing this.  TensorFlow is built to encourage it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainable Parameters: $\\boldsymbol \\theta = (\\theta_0, \\theta_1, \\theta_2) \\in \\mathbb R^3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'theta:0' shape=(3,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "with g.as_default():\n",
    "    theta = tf.get_variable(name=\"theta\", \n",
    "                            shape=[3], \n",
    "                            initializer=tf.random_normal_initializer(mean=0, stddev=1, seed=None, dtype=tf.float32),\n",
    "                            dtype=tf.float32)\n",
    "    print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick check to make sure theta looks right when it's initialized.  If you get all zeroes, check the console for error messages.  Sometimes it fails to allocate GPU memory for whatever reason. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.64781421,  0.41394639, -1.041875  ], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g) as s:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(s.run([theta]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, despite the fact that \n",
    "$$\n",
    "H(y,h_{\\boldsymbol \\theta} (\\mathbf p)) = (y-1) \\log (1-h_{\\boldsymbol \\theta} (\\mathbf p)) - y \\log h_{\\boldsymbol \\theta} (\\mathbf p)\n",
    "$$\n",
    "we're better off mashing everything together and computing the gradient of the loss directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "'''Hypothesis function\n",
    "    Args:\n",
    "        X: design matrix (N x 3)\n",
    "        theta: trainable parameters (3 floats)\n",
    "'''    \n",
    "learning_rate = 0.1\n",
    "with g.as_default():\n",
    "    # Step counter\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "    # Compute the hidden layer output (pre-sigmoid function)\n",
    "    logits = tf.reshape(tf.matmul(data_batch, theta[:,None]), [-1])\n",
    "    \n",
    "    # Compute the loss of each example in the mini-batch\n",
    "    losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(label_batch, tf.float32), logits=logits)\n",
    "    \n",
    "    # Find the mean loss of the mini-batch\n",
    "    loss = tf.reduce_mean(losses)\n",
    "    \n",
    "    # Train Op\n",
    "    train_op = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test train_op in a simple session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for a sanity check, let's run our train_op in an old-fashioned tf.Session.  For the real training, we'll use a tf.train.Supervisor.managed_session() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step   1:\tloss = 1.407199\n",
      "step 101:\tloss = 0.340095\n",
      "step 201:\tloss = 0.133344\n",
      "step 301:\tloss = 0.361933\n",
      "step 401:\tloss = 0.375085\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g) as s:\n",
    "    s.run(tf.global_variables_initializer())\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord, sess=s)\n",
    "    \n",
    "    for i in range(500):\n",
    "        t_op, ls, gstep = s.run([train_op, loss, global_step])\n",
    "        if (i % 100 == 0):\n",
    "            print(\"step %3d:\\tloss = %f\" % (gstep, ls))\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good.  Now let's do it with a pausable session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow supports saving and loading the network parameters using the `tf.train.Saver` class. Since we want checkpoints, however, we can make this easier by using a `tf.train.Supervisor.managed_session` instead of the usual `tf.Session`.  \n",
    "\n",
    "A `managed_session` has its own Saver and it will save checkpoints automatically and reload from them.  The only thing it leaves for us to do is to implement is the \"pause button.\"  To do that, we'll periodically check the training directory for a file named \"pause\".  If one exists, we'll delete it (in preparation for the next use), save a checkpoint manually, and then shut down the training.\n",
    "\n",
    "**BEFORE RUNNING THE NEXT CELL** get a terminal open and have this command ready to go:\n",
    "```\n",
    "touch /tmp/pausable_training/pause\n",
    "```\n",
    "\n",
    "**NB**: Only the *values* of the network parameters are saved and reloaded--not the network topology itself.  We still have to build the graph and if there's input (which there usually is), load it.  The step we skip is the network initialization step (`tf.global_variables_initializer().run()`).  Obviously, we have to leave that to `managed_session()` since we want it to handle the task of choosing whether to reload values from storage or initialize them from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/pausable_training/pt.ckpt-1900\n",
      "INFO:tensorflow:global_step/sec: 0\n",
      "step 2000:\tp = 4000.000000\n",
      "step 2100:\tp = 4200.000000\n",
      "step 2200:\tp = 4400.000000\n",
      "step 2300:\tp = 4600.000000\n",
      "step 2400:\tp = 4800.000000\n",
      "step 2500:\tp = 5000.000000\n",
      "Pause command received.  Saving checkpoint and shutting down.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "training_dir = \"/tmp/pausable_training/\"\n",
    "pause_file = training_dir + \"pause\"\n",
    "checkpoint_file = training_dir + \"pt.ckpt\"\n",
    "\n",
    "with graph.as_default():\n",
    "    sv = tf.train.Supervisor(logdir=training_dir)\n",
    "    with sv.managed_session() as s:\n",
    "\n",
    "        # Supervisor calls tf.global_variables_initializer().run() for us\n",
    "\n",
    "        while not sv.should_stop():\n",
    "            _, y, gstep = s.run([train_op, p, global_step])\n",
    "            if (gstep % 100 == 0):\n",
    "                print(\"step %3d:\\tp = %f\" % (gstep,y))\n",
    "                if (tf.gfile.Exists(pause_file)):\n",
    "                    sv.Stop()\n",
    "                time.sleep(1)\n",
    "                \n",
    "        if tf.gfile.Exists(pause_file):\n",
    "            print(\"Pause command received.  Saving checkpoint and shutting down.\")\n",
    "            tf.gfile.Remove(pause_file)\n",
    "            sv.saver.save(s, checkpoint_file, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we shut down and continue with more training later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Kill the kernel, forcing it to restart - NB: You'll have to step manually from here on.\n",
    "import os\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-run the notebook at this point to perform subsequent training sessions.  Each time it will pick up where it left off (unless the training directory has been wiped).  Note the reported \"step\" numbers above to see it resuming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Objectives\n",
    "#### Make this a little less skeletal \n",
    "Use input data and actually train some simple parameters (call `minimize`) so we have a real train_op.  Try logistic regression on a randomly-generated dataset.\n",
    "#### Get a better handle on TensorBoard reporting  \n",
    "Want to see the loss and the trainable parameters at the very least.  Eventually we'll add learning rates, kernels, etc.\n",
    "#### Keep a roster of the top performers\n",
    "Keep snapshots of the networks corresponding to local minima in the training loss.  Make it easy to load those snapshots.  Once we suspect we're beginning to overtrain the network, we can load up those \"training highlights\", cross-validate them, and package up the best one for \"release\" (e.g. Kaggle submission).\n",
    "#### Use a file input queue\n",
    "We want to operate on lots of data: more then what will fit main memory.  See this page: https://www.tensorflow.org/programmers_guide/reading_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix (under construction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the loss is, therefore,\n",
    "$$\\begin{eqnarray}\n",
    "\\nabla_{\\boldsymbol \\theta} \\ell(\\mathbf p) &=& \\left(\\frac \\partial {\\partial \\theta_0} \\ell(\\mathbf p), \\frac \\partial {\\partial \\theta_1} \\ell(\\mathbf p), \\frac \\partial {\\partial \\theta_2} \\ell(\\mathbf p)\\right)\\\\\n",
    "\\end{eqnarray}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "H(y,h_{\\boldsymbol \\theta} (\\mathbf p)) &= -\\sum_{i=0}^1 P(y=i \\mid y) \\log P(y=i \\mid \\mathbf p; \\boldsymbol \\theta)\\\\\n",
    "&= -(1-y) \\log P(y=0 \\mid \\mathbf p; \\boldsymbol \\theta) - y \\log P(y=1 \\mid \\mathbf p; \\boldsymbol \\theta)\\\\\n",
    "&= -(1-y) \\log \\left(1-h_{\\boldsymbol \\theta} (\\mathbf p)\\right) - y \\log h_{\\boldsymbol \\theta} (\\mathbf p)\\\\\n",
    "&= -(1-y) \\log \\left(1-\\frac 1 {1+e^{-\\langle \\boldsymbol \\theta, \\mathbf p\\rangle}} \\right) - y \\log \\frac 1 {1+e^{-\\langle \\boldsymbol \\theta, \\mathbf p\\rangle}}\\\\\n",
    "&= -(1-y) \\log \\left(\\frac {e^{-\\langle \\boldsymbol \\theta, \\mathbf p\\rangle}} {1+e^{-\\langle \\boldsymbol \\theta, \\mathbf p\\rangle}} \\right) - y \\log \\frac 1 {1+e^{-\\langle \\boldsymbol \\theta, \\mathbf p\\rangle}}\\\\\n",
    "&= (y-1) \\left[ \\log e^{-\\langle \\boldsymbol \\theta, \\mathbf p\\rangle} - \\log \\left(1+e^{-\\langle \\boldsymbol \\theta, \\mathbf p\\rangle}\\right) \\right] + y \\left[ \\log \\left(1+e^{-\\langle \\boldsymbol \\theta, \\mathbf p\\rangle}\\right) \\right]\\\\\n",
    "&= y \\log e^{-\\langle \\boldsymbol \\theta, \\mathbf p\\rangle} - y \\log \\left(1+e^{-\\langle \\boldsymbol \\theta, \\mathbf p\\rangle}\\right) - \\log e^{-\\langle \\boldsymbol \\theta, \\mathbf p\\rangle} + \\log \\left(1+e^{-\\langle \\boldsymbol \\theta, \\mathbf p\\rangle}\\right) + y \\log \\left(1+e^{-\\langle \\boldsymbol \\theta, \\mathbf p\\rangle}\\right)\\\\\n",
    "&= (y-1) \\log e^{-\\langle \\boldsymbol \\theta, \\mathbf p\\rangle} + \\log \\left(1 + e^{-\\langle \\boldsymbol \\theta, \\mathbf p\\rangle}\\right)\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "104px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
