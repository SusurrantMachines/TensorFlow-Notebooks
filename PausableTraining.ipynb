{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop a skeletal framework for long-term, indefinite training that is conducted in multiple, discrete sessions spread out over time.  We want to be able to train a network like this:\n",
    "- Hit the switch to start training.\n",
    "- Open up TensorBoard and see the graph of the loss function, check out the kernels, etc.\n",
    "- Wait for a while, refresh TensorBoard, and see how things are coming along.\n",
    "- Hit some kind of \"pause button\" and shut everything down for a while.\n",
    "- Come back later, fire it up again, and it picks up right where it left off without misssing a beat.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Build the Graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just need dummy operations.  No need to saddle the framework with actual machine learning.  So, we'll use just one trainable parameter called $p$ and we won't bother feeding any input to it.\n",
    "\n",
    "The \"network output\" is simply the constant function $f(x) = p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    p = tf.get_variable(name=\"tf_p\", \n",
    "                        shape=[1], \n",
    "                        initializer=tf.constant_initializer(0),\n",
    "                       dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this at least somewhat realistic (not *too* far off from a real application), we'll make a proper **train_op** using a tf.train.Optimizer.\n",
    "\n",
    "tf.train.**GradientDescent** is the simplest one, so let's go with that.\n",
    "\n",
    "To simulate training, we'll just increment our trainable parameter $p$ by (oh, let's say) 2 at every training step.  The straightforward way to do this would be to use a loss function with a constant gradient of -2 (e.g. $\\mbox{loss}(x) = -2x$).  That would require setting up an input placeholder for $x$, though, and we don't want to clutter up the framework with unnecessary variables.\n",
    "\n",
    "So, instead of calling tf.train.GradientDescentOptimizer.**minimize(loss)**--which takes the gradient of the loss function and then applies it to the trainable parameters--we'll take a detour around the gradient-taking part and call tf.train.GradientDescentOptimizer.**apply_gradients(grad)** directly.  We need to pass it a tensor which it will assume is the gradient of the loss.  It'll be none the wiser if we just give it -2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    \n",
    "    # Step counter\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    \n",
    "    # Fake the gradient of a loss function to make the optimizer think p always needs to be adjusted by +2\n",
    "    fake_loss_gradient = -2 * tf.ones([1])\n",
    "\n",
    "    # Simulate calling compute_gradients() (the first half of minimize())\n",
    "    grads_and_vars = [(fake_loss_gradient, p)]\n",
    "    \n",
    "    # Set train_op to be the apply_gradients part of minimize()\n",
    "    train_op = tf.train.GradientDescentOptimizer(learning_rate=1.0).apply_gradients(grads_and_vars, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Test train_op in a simple session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Just for a sanity check, let's run our train_op in an old-fashioned tf.Session.  For the real training, we'll use a tf.train.Supervisor.managed_session() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step   1:\tp = 2.000000\n",
      "step 101:\tp = 202.000000\n",
      "step 201:\tp = 402.000000\n",
      "step 301:\tp = 602.000000\n",
      "step 401:\tp = 802.000000\n",
      "step 501:\tp = 1002.000000\n",
      "step 601:\tp = 1202.000000\n",
      "step 701:\tp = 1402.000000\n",
      "step 801:\tp = 1602.000000\n",
      "step 901:\tp = 1802.000000\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as s:\n",
    "    \n",
    "    # Initialize the network parameters\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for i in range(1000):\n",
    "        _, y, gstep = s.run([train_op, p, global_step])\n",
    "        if (i % 100 == 0):\n",
    "            print(\"step %3d:\\tp = %f\" % (gstep,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0\n",
      "0 step 5804:\tp = 11608.000000\n",
      "100 step 5904:\tp = 11808.000000\n",
      "200 step 6004:\tp = 12008.000000\n",
      "300 step 6104:\tp = 12208.000000\n",
      "400 step 6204:\tp = 12408.000000\n",
      "500 step 6304:\tp = 12608.000000\n",
      "600 step 6404:\tp = 12808.000000\n",
      "700 step 6504:\tp = 13008.000000\n",
      "You paused!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "training_dir = \"/tmp/pausible_training/\"\n",
    "pause_file = training_dir + \"pause\"\n",
    "with graph.as_default():\n",
    "    sv = tf.train.Supervisor(logdir=training_dir)\n",
    "    with sv.managed_session() as s:\n",
    "\n",
    "        # Supervisor calls tf.global_variables_initializer().run() for us\n",
    "\n",
    "        for i in range(5000):\n",
    "            if tf.gfile.Exists(pause_file):\n",
    "                print(\"Pause command received.  Saving checkpoint and shutting down.\")\n",
    "                tf.gfile.Remove(pause_file)\n",
    "                sv.saver.save(s, training_dir + 'model.ckpt', global_step=global_step)\n",
    "                break\n",
    "            _, y, gstep = s.run([train_op, p, global_step])\n",
    "            if sv.should_stop():\n",
    "                break\n",
    "            if (i % 100 == 0):\n",
    "                print(\"%d step %3d:\\tp = %f\" % (i, gstep,y))\n",
    "                time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Cool!  It can resume from where it left off!\n",
    "The problem now is that it's saving at regular checkpoints--which is great (I think that's great)...  We want to be able to do that.  But we also want to be able to force it to save on command.  That way, when we deliberately interrupt it we can save immediately before halting and not lose anything.  Regular checkpoints are great for unexpected shutdowns, but when we know we're about to shut down, we shouldn't have to rely on the luck of the checkpoint schedule.\n",
    "\n",
    "So, I think the way you have to do a deliberate save is like this:\n",
    "sv.saver.save(s, 'model', global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "We can use tf.train.Supervisor to run a managed session.  \n",
    "\n",
    "There  some kind of *save-and-quit* button.  Not sure how best to do this, but an easy way would be for the code to check for the existence of a file called \"pause\" in the training dir.  To pause the training, you'd go to a bash prompt and do \"touch pause\".  The training code would notice it, delete it (to prepare for the next pause), save a checkpoint, and shut down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next Objective\n",
    "Provide a way to load up the state of the network from earlier points in time.  Local minima in the training loss. Then we can test or validate those and package up the best one for \"release\" (e.g. Kaggle submission)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "104px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
